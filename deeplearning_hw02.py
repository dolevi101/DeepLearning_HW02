# -*- coding: utf-8 -*-
"""DeepLearning-HW02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cy1sFd7n37hub9pIGdse-iYVKqXs14YQ
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.python.client import device_lib
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras import backend as K
from keras.layers.pooling import MaxPooling2D, AveragePooling2D
from keras.layers.core import Lambda, Flatten, Dense
from keras.regularizers import l2
from keras.models import Sequential
from keras.optimizers import *
from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate
from keras.models import Model
from keras.engine.topology import Layer

#from matplotlib import pyplot as plt
import matplotlib.pyplot as plt

from PIL import Image
#from google.colab.patches import cv2_imshow
import os
import zipfile

def unzip(path_to_zip_file):
  with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:
    zip_ref.extractall("./")

def preprocess_image(path, new_shape):
  image = Image.open(path)
  new_image = image.resize((105, 105))
  return np.asarray(new_image).reshape(new_shape)

# this function returns two lists. In the first list every entry is a pair X_i of images
# In the second list every entry is their matching label Y_i:
# indicates whether it is the same person or not.

def build_pairs(dataset_path, instrcut_path, new_shape):
  inputs = []
  labels = []
  with open(instrcut_path, 'r') as file:
    number_true_labels = int(file.readline())
    line_index = 1
    while line_index <= number_true_labels:
      last_line = file.readline()
      name, pic_1, pic_2 = last_line.rstrip('\n').split("\t")
      pic1_path = dataset_path + name + "/" + name + "_" + pic_1.rjust(4,'0') + ".jpg"
      pic2_path = dataset_path + name + "/" + name + "_" + pic_2.rjust(4,'0') + ".jpg"    
      inputs.append([preprocess_image(item, new_shape) for item in [pic1_path, pic2_path]])
      labels.append(1)

      line_index += 1
    
    # the following lines indicates non-matching images
    while True:
      last_line = file.readline()
      if not last_line:
        break
      name_1, pic_1, name_2, pic_2 = last_line.rstrip('\n').split("\t")
      pic1_path = dataset_path + name_1 + "/" + name_1 + "_" + pic_1.rjust(4,'0') + ".jpg"
      pic2_path = dataset_path + name_2 + "/" + name_2 + "_" + pic_2.rjust(4,'0') + ".jpg"
      inputs.append([preprocess_image(item, new_shape) for item in [pic1_path, pic2_path]])
      labels.append(0)   

  return np.array(inputs), np.array(labels)

def initialize_weights(shape, dtype=None):
    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)

def initialize_bias(shape, dtype=None):
    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)

def get_siamese_model(input_shape):
    left_input = Input(input_shape)
    right_input = Input(input_shape)
    model = Sequential()
    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,
                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))
    model.add(MaxPooling2D())
    model.add(Conv2D(128, (7,7), activation='relu',
                     kernel_initializer=initialize_weights,
                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))
    model.add(MaxPooling2D())
    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,
                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))
    model.add(MaxPooling2D())
    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,
                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))
    model.add(Flatten())
    model.add(Dense(4096, activation='sigmoid',
                   kernel_regularizer=l2(1e-3),
                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))
    encoded_l = model(left_input)
    encoded_r = model(right_input)
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])
    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)
    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)
    return siamese_net

def get_batch(data, index, batch_size):
  return np.array(data[index : index + batch_size])

if __name__ == "__main__":
  #unzip("/content/lfwa.zip")
  dataset_path = "/content/lfw2/lfw2/" #the relative path inside my project folder
  train_path = "/content/pairsDevTrain.txt"
  test_path = "/content/pairsDevTest.txt"
  shape = (105, 105, 1)
  batch_size = 64
  num_epochs = 50
  check_test_acc = 1
  index_in_data = 0
  validation_split = 0.2

  train_inputs, train_labels = build_pairs(dataset_path, train_path, shape)
  test_inputs, test_labels = build_pairs(dataset_path, test_path, shape)

  train_set = np.split(train_inputs,2,axis=1)
  train_set = [np.squeeze(item, 1) for item in train_set]

  test_set = np.split(test_inputs,2,axis=1)
  test_set = [np.squeeze(item, 1) for item in test_set]

  #image = train_inputs[150][1]
  #print("shape of image is {}".format(image.shape))
  #cv2_imshow(image)
  
  model = get_siamese_model(shape)
  #model.summary()

  optimizer = Adam(lr = 0.00006)
  accuracy_func = tf.keras.metrics.BinaryAccuracy(name="binary_accuracy", dtype=None, threshold=0.5)
  model.compile(loss="binary_crossentropy",optimizer=optimizer,
                metrics=[accuracy_func])
  
  train_losses = []
  train_accs = []
  test_losses = []
  test_accs = []
  epoch_indexes = []

  
  for iteration in range(num_epochs):
    # evaluate model on test set
    if iteration % check_test_acc == 0:
      print("\nEvaluating...")
      test_loss, test_acc = model.evaluate(x = test_set, y = test_labels)
      print("test loss: {}, test acc: {}".format(test_loss, test_acc))
      test_losses.append(test_loss)
      test_accs.append(test_acc)
      epoch_indexes.append(iteration)
      print("Done Evaluating\n")
    
    print("epoch number {}".format(iteration))
    train_pair_batch = get_batch(train_inputs, index_in_data, batch_size)
    train_label_batch = get_batch(train_labels, index_in_data, batch_size)

    index_in_data += batch_size
    if index_in_data >= train_inputs.shape[0]:
      index_in_data = 0

    train_batch = np.split(train_pair_batch,2,axis=1)
    train_batch = [np.squeeze(item, 1) for item in train_batch]

    #loss = model.fit(x = train_batch, y = train_label_batch)

    history = model.fit(x = train_set, y = train_labels, batch_size=batch_size)
    train_losses.append(history.history['loss'][0])
    train_accs.append(history.history['binary_accuracy'][0])

  
  model.save('/content/saved_model')


  print("\ntrain_losses")
  print(train_losses)

  print("\ntrain_accs")
  print(train_accs)

  print("\ntest_losses")
  print(test_losses)

  print("\ntest_accs")
  print(test_accs)

  print("\nepoch_indexes")
  print(epoch_indexes)

  #fig = plt.figure()
  #ax = plt.axes()
  #ax.plot(test_accs, epoch_indexes)

