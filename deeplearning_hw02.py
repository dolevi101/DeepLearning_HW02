# -*- coding: utf-8 -*-
"""DeepLearning-HW02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cy1sFd7n37hub9pIGdse-iYVKqXs14YQ
"""
import os

import numpy as np
import tensorflow as tf
from PIL import Image
from keras import backend as K
from keras.layers import Conv2D, Input
from keras.layers.core import Lambda, Flatten, Dense
from keras.layers.pooling import MaxPooling2D
from keras.models import Model
from keras.models import Sequential
from keras.optimizers import *
from keras.regularizers import l2


def preprocess_image(path, new_shape):
    """
    Prepares an image
    :param path:
    :param new_shape:
    :return:
    """
    image = Image.open(path)
    new_image = image.resize((105, 105))
    return np.asarray(new_image).reshape(new_shape)


def build_pairs(dataset_path, instrcut_path, new_shape):
    """
    Builds the class pairs for training
    :param dataset_path:
    :param instrcut_path:
    :param new_shape:
    :return:
    """
    inputs = []
    labels = []
    with open(instrcut_path, 'r') as file:
        number_true_labels = int(file.readline())
        line_index = 1

        # For the matching images
        while line_index <= number_true_labels:
            last_line = file.readline()
            name, pic_1, pic_2 = last_line.rstrip('\n').split("\t")
            pic1_path = dataset_path + name + "/" + name + "_" + pic_1.rjust(4, '0') + ".jpg"
            pic2_path = dataset_path + name + "/" + name + "_" + pic_2.rjust(4, '0') + ".jpg"
            inputs.append([preprocess_image(item, new_shape) for item in [pic1_path, pic2_path]])
            labels.append(1)

            line_index += 1

        # For the non-matching images
        while True:
            last_line = file.readline()
            if not last_line:
                break
            name_1, pic_1, name_2, pic_2 = last_line.rstrip('\n').split("\t")
            pic1_path = dataset_path + name_1 + "/" + name_1 + "_" + pic_1.rjust(4, '0') + ".jpg"
            pic2_path = dataset_path + name_2 + "/" + name_2 + "_" + pic_2.rjust(4, '0') + ".jpg"
            inputs.append([preprocess_image(item, new_shape) for item in [pic1_path, pic2_path]])
            labels.append(0)

    return np.array(inputs), np.array(labels)


def prepare_siamese_one_hot_model(input_shape):
    def initialize_weights(obj_shape):
        return np.random.normal(loc=0.0, scale=1e-2, size=obj_shape)

    def initialize_bias(obj_shape):
        return np.random.normal(loc=0.5, scale=1e-2, size=obj_shape)

    left_input = Input(input_shape)
    right_input = Input(input_shape)
    siamese_model = Sequential([
        Conv2D(64,
               (10, 10),
               activation='relu',
               input_shape=input_shape,
               kernel_initializer=initialize_weights,
               kernel_regularizer=l2(2e-4)),
        MaxPooling2D(),
        Conv2D(128,
               (7, 7),
               activation='relu',
               kernel_initializer=initialize_weights,
               bias_initializer=initialize_bias,
               kernel_regularizer=l2(2e-4)),
        MaxPooling2D(),
        Conv2D(128,
               (4, 4),
               activation='relu',
               kernel_initializer=initialize_weights,
               bias_initializer=initialize_bias,
               kernel_regularizer=l2(2e-4)),
        MaxPooling2D(),
        Conv2D(256,
               (4, 4),
               activation='relu',
               kernel_initializer=initialize_weights,
               bias_initializer=initialize_bias,
               kernel_regularizer=l2(2e-4)),
        Flatten(),
        Dense(4096,
              activation='sigmoid',
              kernel_regularizer=l2(1e-3),
              kernel_initializer=initialize_weights, bias_initializer=initialize_bias)])

    L1_dist_lambda = Lambda(lambda tens: K.abs(tens[0] - tens[1]))
    L1_distance = L1_dist_lambda([siamese_model(left_input), siamese_model(right_input)])

    prediction = Dense(1, activation='sigmoid', bias_initializer=initialize_bias)(L1_distance)

    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)
    return siamese_net


def get_batch(data, index, batch_size):
    return np.array(data[index: index + batch_size])


if __name__ == "__main__":
    curr_dir = os.path.dirname(os.path.realpath(__file__))
    dataset_path = os.path.join(curr_dir, "content/lfw2/lfw2/")
    train_path = os.path.join(curr_dir, "content/pairsDevTrain.txt")
    test_path = os.path.join(curr_dir, "content/pairsDevTest.txt")
    shape = (105, 105, 1)
    batch_size = 64
    num_epochs = 50
    check_test_acc = 1
    index_in_data = 0
    validation_split = 0.2

    train_inputs, train_labels = build_pairs(dataset_path, train_path, shape)
    test_inputs, test_labels = build_pairs(dataset_path, test_path, shape)

    train_set = np.split(train_inputs, 2, axis=1)
    train_set = [np.squeeze(item, 1) for item in train_set]

    test_set = np.split(test_inputs, 2, axis=1)
    test_set = [np.squeeze(item, 1) for item in test_set]

    model = prepare_siamese_one_hot_model(shape)

    optimizer = Adam(lr=0.00006)
    accuracy_func = tf.keras.metrics.BinaryAccuracy(name="binary_accuracy", dtype=None, threshold=0.5)
    model.compile(loss="binary_crossentropy", optimizer=optimizer,
                  metrics=[accuracy_func])

    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []
    epoch_indexes = []

    for iteration in range(num_epochs):
        # evaluate model on test set
        if iteration % check_test_acc == 0:
            print("\nEvaluating...")
            test_loss, test_acc = model.evaluate(x=test_set, y=test_labels)
            print("test loss: {}, test acc: {}".format(test_loss, test_acc))
            test_losses.append(test_loss)
            test_accs.append(test_acc)
            epoch_indexes.append(iteration)
            print("Done Evaluating\n")

        print("epoch number {}".format(iteration))
        train_pair_batch = get_batch(train_inputs, index_in_data, batch_size)
        train_label_batch = get_batch(train_labels, index_in_data, batch_size)

        index_in_data += batch_size
        if index_in_data >= train_inputs.shape[0]:
            index_in_data = 0

        train_batch = np.split(train_pair_batch, 2, axis=1)
        train_batch = [np.squeeze(item, 1) for item in train_batch]

        # loss = model.fit(x = train_batch, y = train_label_batch)

        history = model.fit(x=train_set, y=train_labels, batch_size=batch_size)
        train_losses.append(history.history['loss'][0])
        train_accs.append(history.history['binary_accuracy'][0])

    model.save('/content/saved_model')

    print("\ntrain_losses")
    print(train_losses)

    print("\ntrain_accs")
    print(train_accs)

    print("\ntest_losses")
    print(test_losses)

    print("\ntest_accs")
    print(test_accs)

    print("\nepoch_indexes")
    print(epoch_indexes)
